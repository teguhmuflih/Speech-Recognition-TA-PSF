# -*- coding: utf-8 -*-
"""Speech_Recognition_TA _with Google_dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TdMBj-mc60AKOph0wZxlulKBFtAl6z7x
"""

"""Import Library yang dibutuhkan"""

import json
import numpy as np
from sklearn.model_selection import train_test_split
import tensorflow.keras as keras
import matplotlib.pyplot as plt
import tensorflow as tf



"""# Variabel Awal



"""

DATA_PATH = r"dataset_manual_psf.json"
SAVED_MODEL_PATH = "model_manual_dataset.h5"
EPOCHS = 64
BATCH_SIZE = 64
LEARNING_RATE = 0.00001

"""#**Memuat Data dari file suara yang sudah diproses menjadi JSON (*JavaScript object notation*)** """

def load_data(data_path):
    """ memuat dataset training dari file Json,
        :return X (ndarray): Inputs
        :return y (ndarray): Targets
    """

    with open(data_path, "r") as fp:
        data = json.load(fp)

    X = np.array(data["MFCCs"])
    y = np.array(data["labels"])
    return X, y

"""#**Mempersiapkan dataset dengan membagi dataset menjadi *Train*, *Test* dan *Validation*.**"""

def prepare_datasets(test_size, validation_size):
    """Loads data and splits it into train, validation and test sets.
    """

    # load data
    X, y = load_data(DATA_PATH)

    # create train, validation and test split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)
    X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=validation_size)

    return X_train, X_validation, X_test, y_train, y_validation, y_test

"""# Menampilkan diagram hasil training berdasarkan history training model."""

def plot_history(history):
    plt.figure(figsize=(15,5))
    
    # membuat accuracy sublpot
    plt.subplot(121)
    plt.plot(history.history['accuracy'])
    plt.plot(history.history['val_accuracy'])
    plt.title('Akurasi Model')
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc='best')
    
    # membuat error sublpot
    plt.subplot(122)
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('Loss Model')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc='best')
    plt.show()

"""# **Membuat Arsitektur Model untuk di *training*.**"""

def build_model(input_shape):
    """Membuat RNN-LSTM model
    """

    # build network topology
    model = keras.Sequential()

    # 2 LSTM layers
    model.add(keras.layers.LSTM(128, input_shape=input_shape, return_sequences=True))
    model.add(keras.layers.LSTM(64))
    model.add(keras.layers.Dropout(0.25))

    # dense layer
    model.add(keras.layers.Dense(64, activation='relu'))
    model.add(keras.layers.Dropout(0.25))

    # output layer
    model.add(keras.layers.Dense(2, activation='softmax'))

    return model

"""#**Menjalankan modul-modul yang sudah dibuat sebelumnya**"""

if __name__ == "__main__":
    # get train, validation, test splits
    X_train, X_validation, X_test, y_train, y_validation, y_test = prepare_datasets(0.25, 0.2)

    print("Banyak kata NO pada Data Training      :",sum(y_train == 0))
    print("Banyak kata Yes pada Data Training     :",sum(y_train == 1))
    print("Banyak kata NO pada Data Validation    :",sum(y_validation == 0))
    print("Banyak kata Yes pada Data Validation   :",sum(y_validation == 1))
    print("Banyak kata NO pada Data Test          :",sum(y_test == 0))
    print("Banyak kata Yes pada Data Test         :",sum(y_test == 1))

    """Banyak Data pada pembagian dataset Train, Validation dan Test """

    print("banyak data train      :",X_train.shape[0])
    print("banyak data validation :",X_validation.shape[0])
    print("banyak data test       :",X_test.shape[0])

    # create network
    input_shape = (X_train.shape[1], X_train.shape[2]) 
    model = build_model(input_shape)

    """# Menyusun Model"""

    # compile model
    optimiser = keras.optimizers.Nadam(learning_rate=LEARNING_RATE)                                                                                                                                                                                                               
    model.compile(optimizer=optimiser,
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy'])

    model.summary()


    # train model
    history = model.fit(X_train, 
                        y_train, 
                        validation_data=(X_validation, y_validation), 
                        batch_size=BATCH_SIZE,
                        epochs=EPOCHS
                        #callbacks = callbacks
                        )

    """# Gambar Evaluasi Model"""

    # plot accuracy/error pada training and validation
    plot_history(history)

    # evaluasi model pada Training set
    train_loss, train_acc = model.evaluate(X_train, y_train, verbose=2)
    print('Training accuracy:', train_acc)
    print('Training Lost    :', train_loss)

    # evaluate model on Validation set
    valid_loss, valid_acc = model.evaluate(X_validation, y_validation, verbose=2)
    print('Validation accuracy:', valid_acc)
    print('Validation Lost    :', valid_loss)

    # evaluate model on test set
    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)
    print('Test accuracy:', test_acc)
    print('Test Lost    :', test_loss)

    y_pred = np.argmax(model.predict(X_test), axis=1)

    from sklearn.metrics import precision_score, recall_score, f1_score
    print('Precision: %.3f' % precision_score(y_test, y_pred))
    print('Recall: %.3f' % recall_score(y_test, y_pred))
    print('F1 Score: %.3f' % f1_score(y_test, y_pred))

    """#Hasil Confussion Matrix"""

    import seaborn as sns
    confusion_mtx = tf.math.confusion_matrix(y_test, y_pred) 
    plt.figure(figsize=(10, 8))
    sns.heatmap(confusion_mtx, xticklabels=["Buka","Tutup"], yticklabels=["Buka","Tutup"],
                annot=True, fmt='g')
    plt.xlabel('Prediction')
    plt.ylabel('Label')
    plt.show()

    """#Visualisasi Model """

    #memvisualisasi model
    from tensorflow.keras.utils import plot_model
    plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)

    # save model
    model.save(SAVED_MODEL_PATH)

